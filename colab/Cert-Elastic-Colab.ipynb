{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5f7734a3",
      "metadata": {},
      "source": [
        "# Cert-Elastic Colab ワークフロー\n",
        "\n",
        "A100 GPU での評価を想定したノートブックです。上から順番に実行すれば、環境構築から最終評価までを一度で行えます。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a2b346",
      "metadata": {},
      "source": [
        "## ノートブック構成\n",
        "- 1. 初期設定 / Setup\n",
        "- 2. 超軽量ロジック確認テスト (Smoke Tests)\n",
        "- 3. 本番実行 (Full Evaluation)\n",
        "- Appendix: 追加のデバッグユーティリティ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36695d6b",
      "metadata": {},
      "source": [
        "## 1. 初期設定 / Setup\n",
        "Colab セッションを立ち上げた直後は、以下のセルを順番に実行してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d518428f",
      "metadata": {},
      "source": [
        "### 1.1 ハードウェアとバージョン確認\n",
        "GPU が正しく割り当てられているか、主要ライブラリのバージョンを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5634f32",
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -V\n",
        "!pip -V\n",
        "!nvidia-smi || echo \"No NVIDIA GPU\"\n",
        "\n",
        "import platform, torch\n",
        "print(\"torch:\", torch.__version__, \"cuda available:\", torch.cuda.is_available())\n",
        "print(\"python:\", platform.python_version())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004b40a1",
      "metadata": {},
      "source": [
        "### 1.2 Google Drive をマウントして環境を定義\n",
        "実行ログやキャッシュを Drive に保存して、次回以降の Colab 起動を高速化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "befa5ff9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os, pathlib\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_ROOT = '/content/drive/MyDrive'\n",
        "PROJECT_NAME = 'Cert-Elastic'\n",
        "PROJECT_DIR = f'{DRIVE_ROOT}/{PROJECT_NAME}'\n",
        "print('PROJECT_DIR =', PROJECT_DIR)\n",
        "\n",
        "# Hugging Face キャッシュを Drive に置いて再利用する\n",
        "os.environ.setdefault('HF_HOME', f'{DRIVE_ROOT}/.cache/huggingface')\n",
        "os.environ.setdefault('HF_DATASETS_CACHE', f\"{os.environ['HF_HOME']}/datasets\")\n",
        "os.environ.setdefault('TRANSFORMERS_CACHE', f\"{os.environ['HF_HOME']}/hub\")\n",
        "for key in ('HF_HOME', 'HF_DATASETS_CACHE', 'TRANSFORMERS_CACHE'):\n",
        "    pathlib.Path(os.environ[key]).mkdir(parents=True, exist_ok=True)\n",
        "print('HF caches under:', os.environ['HF_HOME'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fccc40e",
      "metadata": {},
      "source": [
        "### 1.3 ワークスペースを用意\n",
        "Drive 上に成果物用ディレクトリを作成し、必要であればローカルからコードを同期します。\n",
        "\n",
        "- まだリポジトリを配置していない場合は `!git clone` や `!rsync` などで Drive の `Cert-Elastic/` にコピーしてください。\n",
        "- 以下のセルは成果物用のサブディレクトリを初期化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fecfa355",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "required = ['runs', 'outputs', 'logs', 'models', 'data']\n",
        "for rel in required:\n",
        "    path = pathlib.Path(PROJECT_DIR) / rel\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "print('Ensured directories:', required)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "941698ab",
      "metadata": {},
      "source": [
        "### 1.4 依存ライブラリのインストール\n",
        "Colab に必要な Python パッケージをインストールします。GPU Wheel は CUDA 12.4 用を利用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3baecc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd \"$PROJECT_DIR\"\n",
        "!python -m pip install -U pip wheel setuptools\n",
        "!pip install -U \"jedi>=0.16\"\n",
        "!pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "import datasets\n",
        "try:\n",
        "    import lm_eval\n",
        "    lm_eval_version = getattr(lm_eval, '__version__', 'unknown')\n",
        "except ImportError:\n",
        "    lm_eval_version = 'not installed'\n",
        "print('datasets:', datasets.__version__)\n",
        "print('lm-eval:', lm_eval_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d46694",
      "metadata": {},
      "source": [
        "### 1.5 Python パスとモジュール読み込み確認\n",
        "`cert_elastic` が正しく import できることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "515b05f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "if PROJECT_DIR not in sys.path:\n",
        "    sys.path.insert(0, PROJECT_DIR)\n",
        "print('sys.path[0:3] =', sys.path[:3])\n",
        "\n",
        "try:\n",
        "    import cert_elastic\n",
        "    print('cert_elastic imported from:', cert_elastic.__file__)\n",
        "except Exception as exc:\n",
        "    print('import error:', exc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04192fab",
      "metadata": {},
      "source": [
        "### 1.6 Hugging Face Hub へのログイン\n",
        "Read 専用トークンを使用して Hugging Face にログインします。トークンは環境変数 `HF_TOKEN` に保持され、キャッシュに保存されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697ea99e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import login, whoami\n",
        "except ImportError as exc:\n",
        "    raise RuntimeError('huggingface_hub が見つかりません。requirements のインストールを確認してください。') from exc\n",
        "\n",
        "if os.environ.get('HF_TOKEN'):\n",
        "    print('HF_TOKEN is already set; skipping interactive login.')\n",
        "else:\n",
        "    token = getpass('Enter your Hugging Face token (read access): ').strip()\n",
        "    if token:\n",
        "        login(token=token, add_to_git_credential=True)\n",
        "        os.environ['HF_TOKEN'] = token\n",
        "        try:\n",
        "            info = whoami()\n",
        "            print('Logged in as:', info.get('name') or info.get('preferred_username'))\n",
        "        except Exception:\n",
        "            print('Login succeeded.')\n",
        "    else:\n",
        "        print('No token provided. Some datasets may require authentication.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "541b10a3",
      "metadata": {},
      "source": [
        "### 1.7 データセットのウォームアップ (任意)\n",
        "Hugging Face Datasets ライブラリから直接ダウンロードし、キャッシュしておきます。既にキャッシュ済みの場合は短時間で終了します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "650a26b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "cache_kwargs = {\"trust_remote_code\": True}\n",
        "token = os.environ.get('HF_TOKEN')\n",
        "if token:\n",
        "    cache_kwargs['token'] = token\n",
        "cache_dir = os.environ.get('HF_DATASETS_CACHE') or os.environ.get('HF_HOME')\n",
        "if cache_dir:\n",
        "    cache_kwargs['cache_dir'] = cache_dir\n",
        "print('cache kwargs:', cache_kwargs)\n",
        "\n",
        "def prefetch(label, fn, allow_fail=False):\n",
        "    try:\n",
        "        fn()\n",
        "        print('[prefetch] ok:', label)\n",
        "    except Exception as err:  # noqa: BLE001\n",
        "        msg = f\"[prefetch] warn: {label} failed -> {err}\"\n",
        "        if allow_fail:\n",
        "            print(msg)\n",
        "        else:\n",
        "            raise RuntimeError(msg) from err\n",
        "\n",
        "prefetch('openai/gsm8k:main', lambda: load_dataset('openai/gsm8k', 'main', **cache_kwargs))\n",
        "for cfg in ['algebra','counting_and_probability','geometry','intermediate_algebra','number_theory','prealgebra','precalculus']:\n",
        "    prefetch(f'EleutherAI/hendrycks_math:{cfg}', lambda cfg=cfg: load_dataset('EleutherAI/hendrycks_math', cfg, split='train', **cache_kwargs), allow_fail=True)\n",
        "prefetch('google-research-datasets/mbpp:sanitized', lambda: load_dataset('google-research-datasets/mbpp', 'sanitized', **cache_kwargs))\n",
        "prefetch('openai/openai_humaneval', lambda: load_dataset('openai/openai_humaneval', **cache_kwargs), allow_fail=True)\n",
        "\n",
        "print('Datasets cached under:', cache_dir or '~/.cache/huggingface/datasets')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d868f045",
      "metadata": {},
      "source": [
        "## 2. 超軽量ロジック確認テスト (Smoke Tests)\n",
        "本番前に配線と依存関係をチェックするための最小構成テストです。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fcacdf3",
      "metadata": {},
      "source": [
        "### 2.1 `run_evaluate_cert.py` の高速テスト\n",
        "bf16 / SDPA 構成で最小本数のプロンプトを評価し、ログが生成されることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44dd3ddd",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd \"$PROJECT_DIR\"\n",
        "\n",
        "MODEL_ID = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "DTYPE = 'bfloat16'\n",
        "LOAD_IN_4BIT = 0\n",
        "ATNN_IMPL = 'sdpa'\n",
        "MAX_NEW_TOKENS = 128\n",
        "EVAL_PROMPTS_N = 32\n",
        "RUN_FAST = 1\n",
        "\n",
        "!python run_evaluate_cert.py \\\n",
        "  --model_id {MODEL_ID} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --load_in_4bit {LOAD_IN_4BIT} \\\n",
        "  --device_map auto \\\n",
        "  --attn_impl {ATNN_IMPL} \\\n",
        "  --max_new_tokens {MAX_NEW_TOKENS} \\\n",
        "  --eval_prompts_n {EVAL_PROMPTS_N} \\\n",
        "  --run_fast {RUN_FAST} \\\n",
        "  --out_dir runs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc478e5",
      "metadata": {},
      "source": [
        "### 2.2 フルパイプラインのスモークテスト\n",
        "Colab 用ドライバースクリプトを `--smoke-test` 付きで実行し、全工程が動作するかを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9463f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd \"$PROJECT_DIR\"\n",
        "!python colab/run_full_eval.py \\\n",
        "  --model-id mistralai/Mistral-7B-Instruct-v0.3 \\\n",
        "  --device-map cuda:0 \\\n",
        "  --attn-impl sdpa \\\n",
        "  --smoke-test \\\n",
        "  --math-probe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db96a520",
      "metadata": {},
      "source": [
        "## 3. 本番実行 (Full Evaluation)\n",
        "ここからが A100 前提の本番パラメータです。必要に応じてモデル ID や生成長を調整してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2247d49c",
      "metadata": {},
      "source": [
        "### 3.1 すべての評価タスクを実行\n",
        "GSM8K / Hendrycks MATH / MBPP / HumanEval を Hugging Face Datasets から直接読み込み、完全な評価を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ead2e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd \"$PROJECT_DIR\"\n",
        "\n",
        "MODEL_ID = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "DTYPE = 'bfloat16'\n",
        "ATNN_IMPL = 'sdpa'\n",
        "EPSILON = 0.02\n",
        "ALPHA = 0.5\n",
        "BETA = 2.0\n",
        "TASKS = 'gsm8k,humaneval,mbpp,hendrycks_math'\n",
        "MAX_NEW_TOKENS = 256\n",
        "EVAL_PROMPTS = 64\n",
        "PAPER_N_ITEMS = 256\n",
        "PAPER_GEN_LEN = 256\n",
        "\n",
        "!python colab/run_full_eval.py \\\n",
        "  --model-id {MODEL_ID} \\\n",
        "  --dtype {DTYPE} \\\n",
        "  --device-map auto \\\n",
        "  --attn-impl {ATNN_IMPL} \\\n",
        "  --epsilon {EPSILON} \\\n",
        "  --alpha {ALPHA} \\\n",
        "  --beta {BETA} \\\n",
        "  --max-new-tokens {MAX_NEW_TOKENS} \\\n",
        "  --eval-prompts {EVAL_PROMPTS} \\\n",
        "  --tasks {TASKS} \\\n",
        "  --paper-n-items {PAPER_N_ITEMS} \\\n",
        "  --paper-gen-len {PAPER_GEN_LEN} \\\n",
        "  --out_dir runs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c68270b",
      "metadata": {},
      "source": [
        "### 3.2 直近の出力を要約\n",
        "`runs/run_*` ディレクトリを確認し、主要な JSON / CSV を冒頭だけ表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d33f24",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd \"$PROJECT_DIR\"\n",
        "import glob, json\n",
        "from pathlib import Path\n",
        "\n",
        "run_dirs = sorted(glob.glob('runs/run_*'))\n",
        "print('recent runs:', run_dirs[-3:])\n",
        "if run_dirs:\n",
        "    latest = Path(run_dirs[-1])\n",
        "    print('latest run:', latest)\n",
        "    for name in ['config.eval.json','summary.json','results.logging.json','cert_checks.csv','lmeval_baseline.json','lmeval_cert.json']:\n",
        "        path = latest / name\n",
        "        if not path.exists():\n",
        "            print(f'{path} not found')\n",
        "            continue\n",
        "        print(f'--- {path} (head) ---')\n",
        "        if path.suffix == '.json':\n",
        "            try:\n",
        "                data = json.loads(path.read_text() or '{}')\n",
        "                preview = json.dumps(data, ensure_ascii=False)[:800]\n",
        "            except Exception:\n",
        "                preview = path.read_text()[:800]\n",
        "            print(preview)\n",
        "        else:\n",
        "            print(''.join(path.read_text().splitlines()[:10]))\n",
        "else:\n",
        "    print('No runs yet.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e833a5f",
      "metadata": {},
      "source": [
        "## Appendix: 追加ユーティリティ\n",
        "必要に応じてロングランのログをファイルに保存したい場合に利用してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e92798b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd \"$PROJECT_DIR\"\n",
        "import os, subprocess, sys\n",
        "from pathlib import Path\n",
        "\n",
        "log_path = Path('runs/lmeval_debug.log')\n",
        "os.makedirs(log_path.parent, exist_ok=True)\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, '-m', 'bench.run_lmeval_cert',\n",
        "    '--model_id', 'mistralai/Mistral-7B-Instruct-v0.3',\n",
        "    '--dtype', 'bfloat16',\n",
        "    '--attn_impl', 'sdpa',\n",
        "    '--epsilon', '0.02',\n",
        "    '--alpha', '0.5',\n",
        "    '--beta', '2.0',\n",
        "    '--tasks', 'gsm8k,humaneval,mbpp,hendrycks_math',\n",
        "    '--fewshot', '0',\n",
        "    '--limit', '0',\n",
        "    '--out_dir', 'runs'\n",
        "]\n",
        "\n",
        "print('Running:', ' '.join(cmd))\n",
        "print('Logging to:', log_path)\n",
        "\n",
        "with open(log_path, 'wb') as fh:\n",
        "    proc = subprocess.Popen(cmd, stdout=fh, stderr=subprocess.STDOUT)\n",
        "    try:\n",
        "        proc.wait()\n",
        "    except KeyboardInterrupt:\n",
        "        proc.terminate()\n",
        "        raise\n",
        "\n",
        "print('Return code:', proc.returncode)\n",
        "if log_path.exists():\n",
        "    tail = log_path.read_text(errors='replace')[-20000:]\n",
        "    print('--- log tail ---')\n",
        "    print(tail)\n",
        "else:\n",
        "    print('log file missing')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
